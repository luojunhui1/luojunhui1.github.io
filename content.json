{"pages":[{"title":"关于","text":"我是谁？ 西南交通大学信息科学与技术学院计算机专业学生 罗俊辉 西南交通大学学生会 青年之声工作站前部长 西南交通大学 ROBOMASTER Heilos 战队视觉组成员 我会啥？ 简单的C++ 简单的Matlab 简单的Python 简单的视觉处理 不会算法 联系我？ QQ: 2479694366","link":"/About/index.html"}],"posts":[{"title":"欢迎来到我的博客","text":"","link":"/index/"},{"title":"Image Preprocessing","text":"噪声处理一般而言，我们将噪声分为以下几类 噪声类别 椒盐噪声 加性噪声 乘性噪声 高斯噪声 椒盐噪声也称为脉冲噪声，是图像中经常见到的一种噪声，它是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素（或是两者皆有）。 加性噪声一般指热噪声、散弹噪声等，它们与信号的关系是相加，不管有没有信号，噪声都存在。 而乘性噪声一般由信道不理想引起，它们与信号的关系是相乘，信号在它在，信号不在他也就不在。 图像去噪的方法有很多种，其中均值滤波、中值滤波、高斯滤波等比较基础且成熟，还有一些基于数学中偏微分方程的去噪方法，基于频域的小波去噪方法。中值滤波，均值滤波以其快速、稳定的特性，应用最为广泛，在OpenCV库中也直接提供了方法。 滤波的计算原理也比较容易理解，这里就不做过多介绍，放上一张效果图说明即可。 实际上，对于噪声的去噪处理实际上很多时候是一种模糊处理，这点从OpenCV中函数的命名就可以看出来，去噪让图片更平滑之外，也损失了一些信息，不过我们一般认为损失的信息无足轻重，而平滑更利于之后的特征提取等操作。 图像增强图像增强有以下两类： 图像增强方法 频域法 空间域法 频域法， 顾名思义，频域法就是把图像从空间域利用傅立叶、小波变换等算法把图像从空间域转化成频域，也就是把图像矩阵转化成二维信号，进而使用高通滤波或低通滤波器对信号进行过滤。采用低通滤波器（即只让低频信号通过）法，可去掉图中的噪声；采用高通滤波法，则可增强边缘等高频信号，使模糊的图片变得清晰。 空间域方法主要包括以下两种常用方法： 空间域方法 直方图均衡化 滤波 关于直方图均衡化能增强图像的原理，在博客 https://blog.csdn.net/weixin_38705903/article/details/88584181 中有详细说明。 基于滤波的方法主要有： 滤波 均值滤波 中值滤波 高斯滤波 图像分割图像分割是把图像分成若干个独立子区域的技术和过程，在计算机视觉的应用，我们关注的可以粗略的分为目标和前景，他们对应图像中特定的、具有独特性质的区域。为分割目标，我们需要将这些区域提取出来，这样才能进行如特征提取、目标识别等进一步的操作。 图像分割可分为这两个类别： 类别 非语义分割 语义分割 非语义分割分为以下几种类别： 非语义分割技术 说明 阈值分割 例如OpenCV中的二值化函数 区域分割 代表性的算法有两种：区域生长和区域分裂合并 聚类 利用样本的相似性，把相似的像素点聚合成同一个子区域 边缘分割 图像在边缘处灰度级会发生突变来对图像进行分割 直方图 通过统计图像中的像素，得到图像的灰度直方图，然后在直方图的波峰和波谷是用于定位图像中的簇 水平集 这个比较复杂，会额外写博客解释 区域生长代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940cv::Mat MainWindow::regionGrowFast(const cv::Mat &amp;src, const cv::Point2i seed, int throld){ //convert src to gray for getting gray value of every pixel cv::Mat gray; cv::cvtColor(src,gray, cv::COLOR_RGB2GRAY); // set every pixel to black cv::Mat result = cv::Mat::zeros(src.size(), CV_8UC1); if((seed.x &lt; 0) || (seed.y &lt; 0)) return result; result.at&lt;uchar&gt;(seed.y, seed.x) = 255; //gray value of seed int seed_gray = gray.at&lt;uchar&gt;(seed.y, seed.x); //grow direction sequenc int grow_direction[8][2] = {{-1,-1}, {0,-1}, {1,-1}, {1,0}, {1,1}, {0,1}, {-1,1}, {-1,0}}; //seeds collection std::vector&lt;cv::Point2i&gt; seeds; seeds.push_back(seed); //start growing while(! seeds.empty()){ //get a seed cv::Point2i current_seed = seeds.back(); seeds.pop_back(); for(int i = 0; i &lt; 8; ++i){ cv::Point2i neighbor_seed(current_seed.x + grow_direction[i][0], current_seed.y + grow_direction[i][1]); //check wether in image if(neighbor_seed.x &lt; 0 || neighbor_seed.y &lt; 0 || neighbor_seed.x &gt; (gray.cols-1) || (neighbor_seed.y &gt; gray.rows -1)) continue; int value = gray.at&lt;uchar&gt;(neighbor_seed.y, neighbor_seed.x); if((result.at&lt;uchar&gt;(neighbor_seed.y, neighbor_seed.x) == 0) &amp;&amp; (abs(value - seed_gray) &lt;= throld)){ result.at&lt;uchar&gt;(neighbor_seed.y, neighbor_seed.x) = 255; seeds.push_back(neighbor_seed); } } } return result;} 区域分割代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;iostream&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;Eigen/Core&gt;#include &lt;Eigen/Dense&gt;#include &lt;opencv2/core/eigen.hpp&gt;#include &lt;string&gt;#include &lt;cmath&gt;using namespace std;using namespace cv;using namespace Eigen;Mat src;Mat result;vector&lt;Rect&gt; rectToDepart;vector&lt;Rect&gt; resultRect;bool isDepart(Rect rect){ MatrixXf matrix; cv2eigen(src(rect), matrix); //cout &lt;&lt; matrix.mean() &lt;&lt;&quot;,&quot;&lt;&lt; rect.width&lt;&lt; &quot;,&quot;&lt;&lt;rect.height&lt;&lt;endl; MatrixXf averageRect = MatrixXf::Constant(rect.height, rect.width, matrix.mean()); MatrixXf subm = averageRect - matrix; long coff = ((subm.array() * subm.array()).sum()); if (coff &gt; 1000|| coff&lt;0)return true; return false;}bool Depart(Rect&amp; rect){ if (rect.width&lt;5||rect.height&lt;5 || !isDepart(rect)) { rectangle(result, rect, Scalar(255)); resultRect.push_back(rect); return false; } Mat cur = src(rect); int rectX = rect.x; int rectY = rect.y; int rectHalfWidth = (int)rect.width / 2; int rectHalfHeight = (int)rect.height / 2; rectToDepart.push_back(Rect(rectX, rectY, rectHalfWidth, rectHalfHeight)); rectToDepart.push_back(Rect(rectX + rectHalfWidth, rectY, rect.width - rectHalfWidth, rectHalfHeight)); rectToDepart.push_back(Rect(rectX, rectY + rectHalfHeight, rectHalfWidth, rect.height - rectHalfHeight)); rectToDepart.push_back(Rect(rectX + rectHalfWidth, rectY + rectHalfHeight, rect.width - rectHalfWidth, rect.height - rectHalfHeight)); return true;}int main(){ src = imread(&quot;1.jpg&quot;,IMREAD_GRAYSCALE); result = Mat(src.size(), CV_8UC1,Scalar(0)); Rect src_rect = Rect(0, 0, src.cols, src.rows); Rect cur_rect; Depart(src_rect); while (!rectToDepart.empty()) { cur_rect = rectToDepart.back(); rectToDepart.pop_back(); Depart(cur_rect); } while (1) { imshow(&quot;gray&quot;, src); imshow(&quot;depart&quot;, result); waitKey(30); } return 0;} 图像增广 图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。 目前常用的图像增广技术有如下几种： 图像增广技术 镜像变换 旋转 裁剪 平移 亮度修改 添加噪声 剪切 变换颜色 这些概念的理解比较简单，具体实现在OpenCV库中也有对应的函数可使用，就不详细说明，有兴趣的可以自行去其他地方查阅。","link":"/ImagePreprocessing/"},{"title":"C Plus","text":"","link":"/cplus_index/"},{"title":"数据库_并发","text":"事务并发的可串行化 事务的串行执行（serial execution）:DBMS按顺序一次执行一个事务，各个事务之间没有交错。 事务的并行执行（concurrent execution）:DBMS同时执行多个事务，DBMS对各事务的操作进行调度，另其交错执行。 事务并发执行的好处： 改善系统资源利用率 改善短事务响应时间 事务并发执行可能产生的问题： 多个事务同时存取数据库中统一数据可能会引起·数据的读写冲突，对数据库的一致性造成破坏。 调度的要求： 一个事务的操作在调度中的顺序应与它们在事务中的顺序一致。 事务如不加控制地并发执行，可能会产生以下三个问题： 不加控制的并发执行可能产生的问题 类型 丢失更新 写写冲突 读脏数据 写读冲突 读值不可复现 读写冲突 ​ 丢失更新图例 ​ 读脏数据图例 ​ 读值不可复现图例 由以上三个可能出现的问题得出结论： 把不同事物中对同一对象进行WW\\RW\\WR操作进行交错时都会造成冲突，因此，在安排事务的交错（并发执行）时，应避免将其放在一起。 并发执行的串行化准则 并发原则：既要交错执行，以充分利用系统资源，加快短事务响应时间，又要避免访问冲突。 事物调度原则：调度中，不同事务的操作可以交错，同一事务的操作需保持相对顺序不变。 可串行化的两个要点： 关心对数据库的读写操作 目的是研究如何形成一个可串行化的并发调度，即“等价于”一个串行调度的并发调度。 前趋图：前趋图是一有向图G=(V,E)，V为顶点集合，表示所有参与调度的事物，E可通过分析冲突操作来决定。以下是示例： 事务集：{T1, T2, T3, T4} S= $$W_{T3}(y)R_{T1}(x)R_{T2}(y)W_{T3}(x)W_{T2}(x)W_{T3}(z)R_{T4}(z)W_{T4}(x)$$ $$R_{T2}(y)$$与$$W_{T3}(y)$$有写读冲突（读脏数据），画一条由点T3指向T2的有向边。 $$W_{T2}(x)$$与$$R_{T1}(x)$$有读写冲突，画一条由点T1指向T3的有向边。 $$W_{T2}(x)$$与$$R_{T1}(x)$$和$$W_{T3}(x)$$分别有冲突，画一条由点T3指向T2的有向边和T1指向T2的有向边。 以此类推，最终得到图如下： 判断是否可串行化：若前趋图有回路，则不可串行化；若没有回路，则可以串行化。 寻找等价串行调度方法：先执行入度为0的节点（事务），再将该节点（事务）及其相连边从前趋图中删除，再寻找入读为0的节点（事务）执行，反复如此，直到事务全部执行完毕。 基于锁的并发控制在实际运用中，用前面提到的方法来先验事务的可串行化并生成对应的事务执行顺序是不现实的。主要原因有以下两点： 事务集随机，不固定 不可能实现定好调度 并发控制通常的实现方法：不关心具体的调度，由DBMS制定号=好一个协议，按此协议执行事务，即可保证事务的可串行化。 加锁协议（Locking Protocol）:是上面所说的协议的一种，利用加锁实现并发控制，即在操作前对操作对象加锁。 加锁协议类型 特性 X锁（排他锁） 可读可写，其他事物不可对被加锁的数据操作，直到X锁被释放。避免了所有类型的冲突，但降低了并发度。 两阶段加锁协议（2PL协议） 所有加锁都在释放锁之前。若所有事物都遵循该协议，则它们的任何调度都是可串行化的。 （S,X）锁 与X锁相比，多了一条S锁与S锁相容，提高了并发度。 （S,U,X）锁 数据对象加了U锁后仍可被加S锁，只在最后写入时，将U锁升级为X锁。相比于X（S,X）锁，又进一步提高了并发度。 多粒度加锁或封锁 数据对象的大小可以是一张表，也可以是一个元组。 级联回退：即一事务还未结束就把锁释放了，又由于其他原因，该事务需要回退，为避免其它事务督导脏数据，要求读了和对该数据更新的操作也回退。在X锁，（S,X）锁，（S,U,X）锁中均存在该问题。 加锁协议补丁1：为避免级联回退，不管是写操作锁还是读操作锁，都应该保持到事务结束才释放。该补丁对其后的加锁协议也适用。 活锁：在（S,X）锁中，若一个数据对象被加S锁，此时若有其它事务对其加S锁，则根据相容矩阵，应该被允许，此时若有事务欲对该数据加X锁，则此X锁会被不断推迟。 加锁协议补丁2：先申请先服务，避免活锁现象造成响应时间过长，影响体验。 粒度、控制和并发度的关系：粒度越大，控制越简单，并发度越低。 多粒度加锁-锁冲突检测问题： 显式加锁：自身被加锁 隐式加锁：上级被加锁 过程： 检查本身，有无显式锁与本事务显式锁冲突。 检查其所有祖先，以防本事务显式锁与其他事务隐式锁冲突。 检查其所有子孙，以防本事务隐式锁与其他事务显式锁冲突。 简化锁冲突的检测： 三种意向锁 意向共享锁（IS锁） 意向排他锁（IX锁） 加锁时，对祖先按自上而下的顺序加意向锁。 解锁时，按自下而上的次序解锁。 死锁检测与预防死锁：当事务出现循环等待时，若不加以干预，则会一直等待下去，形成死锁。 对付死锁的两个办法： 防止死锁 检测死锁，发现死锁后处理死锁。 预防死锁的方法： ​ 给每个事务以时间戳先后为准设立优先级，时间戳越前，事务优先级越高。若有A事务持有资源Z的锁，而B事务申请对Z的锁。若A优先级更高，则B等待；若B优先级更高，则A回滚，再重启事务A并继承A回滚前的时间戳。 检测死锁的方法： 超时法 等待图法 死锁处理的方法： 处理思想：杀死事务，打破循环等待 选择牺牲事务的标准： 选择最迟交付的 选择获得锁最少的 选择回退代价最小的 不同DBMS的标准可能不一样。 被杀死事务的处理方法： 告知用户事务因死锁被杀死，稍后再次向系统交付该事务 由DBMS重启该事务。 不管哪种处理方式，被杀死的事务都要等待一段时间才能被重新执行，否则可能会再次引起死锁。 附图","link":"/%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%B9%B6%E5%8F%91/"},{"title":"水平集算法(level-set)","text":"显式轮廓法在说 level set 之前，我们先谈曲线的演变。假设我们现在有一条曲线如下，为知道这条曲线接下来的演化方向，我们需要知道这条曲线上每个点的运动方向和大小，这样就能确定曲线在下一时间点的轮廓。对于这条曲线来说，这可能并不难，但对于更加一般的情况来说，可能会出现以下几个问题： 显式轮廓法的缺陷 演化过程中轮廓可能需要插值增加轮廓点，或者减少轮廓点，实际应用比较麻烦 轮廓在演化过程中可能会出现分裂，合并的情况，而这在二维函数上难以处理 上面的这种方法很直观，但实现起来困难重重。为解决这两个问题，有人提出了隐式轮廓法。 隐式轮廓法隐函数为理解隐式轮廓法，首先要了解隐函数。我们要描述直线上的两个点，那么可以直接就写出两个点的坐标： 你也可以绕一下，写成集合的形式，也就是所谓的 level set： 这里的 y 的取值就是 level，那么和上面相比，直接是X 轴上的两个点。在这里的话，表达方式就成了，level 为1 的所有x取值的集合。 对于曲线来说，和上面的例子类似，不过曲线是二维的，它可以表示为f(x,y) = 0,于是，曲线的演化可以转化成曲面的演化，而曲面的演化导致了level = 0的曲线的演化。接下来，我们说的就是曲面的演化。 演化一个表面S，而不是一条曲线C，且我们用隐式轮廓法得到的轮廓的定义为这个表面S在高度h=0的所有点构成的集合。于是根据定义，水前曲线就是零水平集φ=0。如下图所示，是从一个演化的表面得到轮廓。以下是演化表面轮廓的过程。 可以看到，在初始时t=0,水平集的轮廓为一个矩形；在t=50的时候，轮廓由3个闭环开始合并为一个闭环，而在t=52时，轮廓结束了合并；在t=90时，轮廓开始分类，t=120时轮廓结束分裂。 水平集方程前面的部分都是直观地介绍一下大致原理，接下来就是玄学的数学理解过程。对于平面上的一个点来说，我们可以这样表示x = (x,y),那么点x属于一个随时间演化的曲线x(t)，而对于x(t)来说，它也会随时间变化，我们将x(t)随t的变化表示成一个新的函数：$\\phi(x(t),t)$,并且定义我们的轮廓就在$\\phi(x(t),t)$ = 0的平面上。 那么什么是$\\phi(x(t),t)$呢，事实上，任何符合$\\phi(x(t),t)$ = 0的曲线和我们的初始轮廓相同的$\\phi(x(t),t)$都可以。 在时刻t=0处，给定初始函数$\\phi$，根据运动方程∂s/∂t我们可以得到任意t时刻的s。对于此，利用链式法则，有： 把$\\frac{\\partial\\phi}{\\partial x}$记作$\\nabla \\phi$,$x_t$ 的方向由表面的法向量给定 ，因此$ x_t = F(x)\\vec{n},\\vec{n} = {\\nabla\\phi \\over{\\phi}}$ 上面的运动方程刻重写为： 最后一个方程定义了φ的运动。给定t=0时的φ以及它随时间演化的运动方程，我们可以通过演化初始函数 $\\phi(x,y,t = 0)$ 得知任何t时刻的 $\\phi(x,y,t)$, 样我们就回答了最初的问题，即我们知道了$\\phi $是什么。 这样我们就回答了最初的问题，即我们知道了$\\phi$是什么。 $\\phi $有个有趣的特征，就是我们可以用下式得到表面的曲率： 我们可以用它来控制平滑度。 说实话，没看懂，看懂了再写。 老翻译家了，这个资料太少了，YouTube上有讲解视频，但那英语真难顶。 参考资源 https://profs.etsmtl.ca/hlombaert/levelset/#fig:levelset-square https://www.cnblogs.com/hyb221512/p/9415995.html 水平集——那些我膜拜过的牛人2[链接实在太长] https://www.zhihu.com/question/22608763","link":"/%E6%B0%B4%E5%B9%B3%E9%9B%86%E7%AE%97%E6%B3%95/"},{"title":"计组_中央处理器","text":"CPU的功能 功能 说明 指令控制 产生下一条指令在内存中的地址、取指令 操作控制 将指令分解成一系列微操作控制信号，控制各部件完成指令所要求的动作 时序控制 对指令的各个微操作实施时间的定时，使它们能按时间先后顺序来执行 数据加工 算术运算、逻辑运算 中断处理 处理异常情况和特殊请求 其它 如总线处理 根据上表，我们能看到：CPU = 控制器 + 运算器 计算机指令控制过程 取指令的过程可用以下流程图表示： CPU的组成 内部主要部件 运算部件ALU 寄存器 控制部件（时序部件和微操作信号发生器） 中断控制逻辑 CPU数据通路 控制总线 数据总线 地址总线 ALUALU基本组成： 输入数据通过多路选择器来选择不同的数据或者是锁存器（寄存器），输出数据可通过移位器取出不同的结果。 寄存器 寄存器类型 说明 通用寄存器 存放原始数据和运算结果，用户可编程访问 专用寄存器 例如PC,IR，暂存器（用来暂存信息，用户不能直接访问，如源寄存器），状态寄存器PSW(用于指示CPU工作方式、算逻辑指令运算结果特征等) 用作主存接口的寄存器 例如地址寄存器AR(MRA)、数据寄存器DR(MDR) 控制单元CU控制单元主要包括： 时序部件 微操作信号发生器 时序部件一般包括： 脉冲源 启停控制逻辑 时序信号发生器 用于产生工作周期、节拍、脉冲等时间信号标志。 微操作信号是最基本的控制命令， 产生微操作的基本依据如下： 时序信号 指令代码（操作码，寻址方式，寄存器号等） 状态（CPU内部的PSW和外设的状态等） 外部请求（控制台请求、外部中断请求、DMA请求等） 根据微操作信号形成的方式，可以将控制分为 组合逻辑控制器；速度快，但电路复杂。 微程序控制器速度稍慢，但电路规整易扩充 CPU内部数据通路结构 单组内总线数据通路结构 多组内总线结构 CPU芯片主要技术参数 字长 内部工作频率 外部工作频率；内频=外频*倍频 前端总线频率（前端总线：连接内存显卡等，是CPU与外界交换数据的主要通路） 地址总线宽度（决定可访问的最大物理空间） 数据总线宽度 …… 时序控制方式与时序系统时序控制方式 同步控制方式 各项微操作都由固定的，统一的时序进行控制。 特点：控制方式简单，容易实现，但存在时间浪费 应用：CPU内部或设备内部 异步控制方式 各微操作按其需要选择不同的时间间隔，不受统一的时间约束；各微操作间的衔接与各部件间的信息交流采用应答方式。 特点：没有时间浪费，提高了效率，但控制复杂 应用：系统总线操作控制 准同步控制方式（总线上大部分采用该方法） 异步方式的同步化（只在节拍结束时查询异步应答信号） 联合控制方式 同步控制方式下的多级时序系统 时序层次划分 指令周期：从取值到指令结束所需时间 三级时序划分：工作周期（机器周期）、节拍（时钟周期）、节拍脉冲（工作脉冲、定时脉冲） 二级时序划分：节拍、节拍脉冲 工作周期与节拍的信号生成 每个工作周期都用一个触发器与之对应 在一条指令运行的任何时刻，只能处于一种工作周期，因此，有且只有一个触发器被置0。 T3用于切换不同的工作周期，T3节拍下降沿可切换工作周期。（该图红色标注有部分错误，不影响主要理解） 节拍信号发生器 以统一节拍法为例 通过时钟来变化环形移位寄存器内的数，实现T0-T3的循环变化。 指令的微操作序列这部分内容只要抓住：CPU取指令、计算、IO几个主干进行理解、实例化内容即可掌握。 模型机 硬件结构（简化PC）16位机 （1）总线：系统总线、内部总线；两者通过AR、 DR连接。 （2）控制器 （3）运算器 （4）寄存器 （5） 内存和I/O设备 注： 由PC向AR发送地址可分为两步：PC-&gt;IB;ARin.后一步不能忘。 ALU做减法运算，下面那个是被减数。 模型机指令系统及寻址方式 （1）指令系统 双操作数指令 双操作数指令的10-15位为操作码，8-9位不用，7位表示两操作数所在寄存器的源操作数和目的操作数顺序，4-6位对应一个操作数的寄存器寻址或寄存器间接寻址方式（8种情况）。3位是指令单/双字节指示，0-2说明另一个操作数的寻址方式（8种情况）。若双子节指示为1，则需一个额外的字存储偏移量等数据。 模型机的时序系统与控制方式 和上面提到的CPU的时序系统与控制方式差不多，这里的共组周期认为有5个，新增可插入DMA周期（DMAC）和可插入中断周期INTC。CPU内部采用同步控制，CPU与MM和I/O之间采用准同步。 16位模型机取指令过程及说明 第一步：将PC中的指令的地址发送到AR中。 第二步：CU发出读内存命令/MMRD，同时将AR中的指令地址送到MAR中；PC寄存器指针转到下一条指令的地址（16位模型机所以是PC+2）。 第三步：将在内存中读好的指令码传入DR中。 第四步：将DR中的指令码传入IR中。","link":"/%E8%AE%A1%E7%BB%84-%E4%B8%AD%E5%A4%AE%E5%A4%84%E7%90%86%E5%99%A8/"},{"title":"数据库_关系数据库设计理论","text":"关系规范中的函数依赖理论 数据依赖分类 说明 函数依赖 包括平凡函数依赖、非平凡函数依赖、完全函数依赖、部分函数依赖、传递函数依赖、等价 多值依赖 无 连接依赖 无 解决依赖的方式： ​ 利用规范化理论，对关系模型进行相应的分解，以消除这些异常。 函数依赖函数依赖的定义：$$若有一个关系模式R(A_1,A_2,…,A_n)，X和Y为\\{A_1,A_2,…,A_n}的子集，r是R中的任意具体关系，\\t_1,t_2是r中的任意两个元组。如果t_1[X]=t_2[X]\\可以推导出t_1[Y]=t_2[Y]，则称X函数决定Y或Y函数\\依赖于X，记作X → Y。$$最小函数依赖集： 定义： 如果F满足下列条件，则称F是最小的函数依赖集。 ​ ① F中每个函数的右边都只有一个属性。 ​ ② 对F中的任何函数依赖A→B，都不存在A的一个真子集C，使函数依赖C →B代替函数依赖A→B后得到和原来的F等价的一组依赖。 ​ ③ 从F中移出任何一个函数依赖都无法再得到和原来的F等价的一组函数依赖。 计算最小函数依赖的方法： 用分解法则。是F中的任何一个函数依赖右部仅含一个属性。 去掉多余的函数依赖。 去掉依赖左部多余属性。 计算最小函数依赖示例： 关系中的1 NF、2 NF、3 NF关系规范化的概念： ​ 关系规范化是一种形式化的技术，它利用主键和候选键以及属性之间的函数依赖来分析关系，这种技术包括一系列作用于单个关系的测试，一旦发现某关系未满足规范化要求，就分解该关系，直到满足规范化要求。 关系模式中的键 候选键：设K为R(U,F)中的属性或属性组，若 ，则K为R的候选键。（K为决定R中全部属性值的最小属性组）。 主键 全键：候选键为整个属性组。 主属性与非主属性： 在R(U,F)中，包含在任一候选键中的属性称为主属性，不包含在任一候选键中的属性称为非主属性。 外键：若R(U,F)的属性（组）X（X属于U）是另一个关系S的主键，则称X为R的外键。（X必须先被定义为S的主键）。 1 NF(最基本的范式) ​ 设R是一个关系模式。如果R的每个属性的值域，都是不可分的简单数据项的集合（即每个属性都不是多值属性也是不复合属性），则称R为第一范式关系模式，记做1 NF。即1 NF要求表中无重复值的列，或表中每一行列交叉处只有一个值。 2 NF 如果R(U,F) ∈1 NF，并且R中的每个非主属性都完全函数依赖于主键，则R(U,F) ∈2 NF 。 从以上定义可以看出，若某个第一范式关系的主键只由一个列组成，则这个关系就是第二范式关系。但如果某个第一范式关系的主键由多个属性列共同构成复合主键，并且存在非主属性对主键的部分函数依赖，则这个关系就不是第二范式关系。 用模式分解的办法可以将非第二范式关系分解为多个第二范式关系。去掉部分依赖关系的分解过程如下： ① 用组成主键的属性集合的每一个子集作为主键构成一个关系。 ​ ② 将依赖于这些主键的属性放置到相应的关系中。 ​ ③ 最后去掉只由主键的子集构成的关系。 3 NF 如果R(U,F) ∈2 NF，并且所有的非主属性都不传递依赖于主键，则R(U,F) ∈3 NF 。 关系数据库的设计目的是消除部分依赖和传递依赖，因为这些依赖会导致更新异常。第二范式和第三范式都不允许存在对主键的部分依赖和传递依赖，但这些定义并没有考虑对候选键的依赖问题。如果只考虑对主键的依赖关系，则在第三范式的关系中可能存在引起数据冗余的函数依赖。第三范式的这些不足导致了另一种更强范式的出现，即Boyce-Codd范式。 BC 范式（Boyce-Codd 范式）如果R(U,F) ∈1 NF，若X→Y且Y￠X时X必包含候选键，则R(U,F) ∈BCNF。 通俗地讲，当且仅当关系中每个函数依赖的决定因子都是候选键时，该范式即为Boyce-Codd范式。 3 NF和BCNF之间的区别在于对一个函数依赖A→B, 3 NF允许B是主键属性，而A不是候选键，而BCNF则要求在这个依赖中，A必须是候选键。因此BCNF也是3 NF，只是更加规范。大多数情况下， 3 NF关系都是BCNF的，只有在特殊情况下，才会发生违反BCNF的情况： ​ ① 关系中包含两个或更多的复合候选键； ​ ② 候选键有重叠，通常至少有一个重叠的属性。","link":"/%E6%95%B0%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E7%90%86%E8%AE%BA/"},{"title":"互联网搜索引擎","text":"总线12345678graph TBA(文本预处理)--&gt;B(词典及检索)C(单个词项查询)--&gt;D(短语查询)D--&gt;E(查询-文档匹配评分)B--&gt;EE--&gt;F(聚类)H(相关反馈)--&gt;GF--&gt;G(文本分类) 文本预处理 预处理项 文本标记化 删除停用词 分词 词条归一化 词形还原 语言对搜索的影响 文本的预处理操作很多都依赖于特定的语言。例如，对于中文或日语而言，没有词与词之间的空格，对于阿拉伯语而言，它的阅读顺序从右到左，但遇到数字又从左到右。对不同语言的分词、标记化和其它预处理操作都需要针对性地做出适应。 停用词 一般文本中大约30%左右的内容都是停用词，即传达信息极少的那一类词，例如“的”，“是”，“我”，“嗯”，诸如此类，这些词语按道理来说性价比过低。但现在的趋势越来越偏离删除停用词的方向，因为在很多情形下，尤其是我们对精确搜索的需求，我们需要停用词来表示更精确的信息，例如歌名，短语，有关联的事务，在搜索它们的时候倘若忽略停用词，得到的结果可能就不尽人意。 归一化 对词语的标准化也是为了减少搜索的负担，对于英语来说，可能U.S.A和USA表达的意思在大部分情境下完全一样，搜索引擎没有必要将二者区分，而对于德语或者其他语言来说，可能会有音调符号、变音符，这些也需要处理，因为在大部分使用情景中，人们并不会标注变音符。对于同义词，比如car和automobile,它们表示的意思基本相同，所以当用户搜索car时，我们也需要查找对应的automobile的内容，对于那些只是有单复数变化，或者所有格变化的，我们也应该做同样的处理。而对于同音词（Soundex）和拼写错误(词独立校正法：编辑距离、k-grams;上下文敏感校正法)的情况，我们也需要做出相应的反应。你可能注意到了，这也要求对不同语言做到个性化处理，在英文和德语中，同一个字母组成的词语，意义却可能风牛马不相及。此外，归一化通常还包括将字母置为小写，将日期形式统一等等。 词干还原 词干还原的重要基础就是分词技术，英文中存在着大量的派生词，通过分词技术我们尽量将那些添加了后缀、前缀的词语都合并到一个词根项中（词形还原），减少搜索的负担。 短语查询在实际的搜索过程中，我们通常不会只用一个词语来搜索，我们有时会使用一个短语来查询。对于单词查询，我们只需要在文档中匹配相应的单词即可，对于有多个单词、单词之间还有联系的短语查询，有其他的方法来处理。 双词索引 将短语中的中的各个单词组合形成新的项，例如：在短语“stanford university palo alto”中，将“standford university”和“university palo”、“palo alto”添加到terms中，我们也可以更进一步，不仅仅只是组合临近的单词，而将每个可能组合成合理单词的组合添加到terms中。但这种方法有着伪正例和索引空间激增的问题（词典中词项数目增多），包含这些所有terms的文档未必真满足短语查询条件。§双词索引方法并不是一个标准的做法 (即倒排索引中一般不会全部采用双词索引方法)，但是可以和其他方法混合使用。 带位置信息索引 倒排记录表中，对每个term在每篇文档中的每个位置(偏移或者单词序号)进行存储:$$&lt;term:出现term文档的篇数\\doc1:位置1，位置2、、位置3\\doc2:位置1，位置2、、位置3\\doc1:位置1，位置2、、位置3\\…………………….\\&gt;$$对于输入的短语查询，需要在文档的层次上进行迭代(不同位置上)合并。不仅仅是简单合并，还要考虑位置匹配。可以使用临近查询，限制某些词项之间的前后距离不大于某值来匹配，而这对双词项并不适用。 词典及容错式检索 词典是指存储词项词汇表的数据结构，词项词汇表(Term vocabulary): 指的是具体数据。对于每个词项，需要存储词项频率和指向倒排记录表的指针，若词项信息采用定长方式存储，则存储图示如下： 为快速定位词项，我们通常使用哈希表和二叉树（B树）两种数据结构来查询： 哈希表： 优点: 在哈希表中的定位速度快于树中的定位速度 查询时间是常数 缺点： 没办法处理词项的微小变形 (resume vs. résumé) 不支持前缀搜索 (比如所有以automat开头的词项) 如果词汇表不断增大，需要定期对所有词项重新哈希 Levenshtein distance 两个字符串s1和s2编辑距离是指从 s1 转换成s2所需要的最短的基本操作数目，基本操作包括插入、替换、删除和交换。 除以上内容外，还有轮排索引、k-gram 索引的内容，但并非重点,不做详细介绍。 轮排索引过程 将查询进行旋转，将通配符旋转到右部 同以往一样查找B-树 问题：相对于通常的B-树，轮排树的空间要大4倍以上 (经验值) 查询-文档匹配评分 Jaccard 系数$$JACCARD(A,B) = \\frac{A\\cap B}{A\\cup B}$$ 不足： 不考虑词项频率 ，即词项在文档中的出现次数 罕见词比高频词的信息量更大，Jaccard系数没有考虑这个信息 没有仔细考虑文档的长度因素 词项频率 词项t的词项频率 $$tf_{t,d}$$ 是指t 在d中出现的次数,但原始的$$tf_{t,d}$$不太合适，因为相关度并不和频率成正比关系，所以我们引入对数词频$$w_td= \\begin{cases}1+\\log_{10}tf_{t,d}, &amp;\\mbox{if } tf_{t,d&gt;0}\\0, &amp;\\mbox{otherwise}\\end{cases}$$除词项频率tf之外，我们还想利用词项在整个文档集中的频率进行权重和评分计算,因为罕见词项比常见词所蕴含的信息更多，某篇包含该词项的文档很可能相关，于是，我们希望罕见词项将有较高权重。为此，我们引入文档频率(Document frequency, df)，$$idf_t=log_{10}\\frac{N}{df_t}$$综上，词项t在文档d中的权重我们可以采用下下式计算：$$w_{t,d}=1+\\log_{10}tf_{t,d}\\times log_{10}\\frac{N}{df_t}$$基于以上的内容跟，我们可以将每篇文档表示成一个基于tfidf权重的实值向量 ∈ R|V|.，于是，我们有一个 |V|维实值空间，空间的每一维都对应词，文档都是该空间下的一个点或者向量，对于Web搜索引擎，这个向量空间会上千万维，对每个向量来说又非常稀疏，大部分都是0。 我们对查询做同以上的同样处理，将查询表示成与文档同一维空间的向量，我们就可以通过比较查询向量和文档向量之间的距离来比较二者的相似度。距离的计算我们不采用欧氏距离，因为它对向量长度很敏感 ，我们可以通过比较夹角来比较相似度，这里的夹角的意义大概等同于查询和文档中的词项分布情况，分布情况越接近，夹角就越小。由于cosin函数在[0,pi]上是单调递减函数，我们可以比较两向量的cos值来比较夹角。 回转归一化 但是余弦归一化更倾向于短文档，即对短文档产生的归一化因子太大，而平均而言对长文档产生的归一化银子太小。我们可以通过找到一个平衡点，对余弦归一化操作进行线性调整使短文档相似度降低，长文档相似度增大来去除原来余弦归一化偏向短文档的问题。 精确TOP K检索及其加速办法 目标：从文档集的所有文档中找出个里查询最近的文档。 一般步骤：将每个文档按评分（余弦相似度）从高到低排序，取前K个结果。 加速策略: 加快余弦相似度的计算 一般在高维空间下，没有很高效的计算余弦的方法。如果查询很短，假设每个查询词项只出现一次，即查询词项无权重，可以通过省略查询向量归一化的方法轻微简化余弦相似度计算方法。 不对所有评分结果排序 通堆方法选出TOP K,堆的构建需要2 J次操作，选出前K个结果，需要2log J步，这里J表示文档总个数。 不计算所有文档的得分 提前终止计算 找一个文档集合A,K&lt;|A|&lt;&lt;N,用A中的TOP K结果代替整个文档集的TOP K结果 仅考虑高idf的词项，低idf代表着出现的次数多，蕴含的信息也少，只考虑高idf词项可以有效减少计算文档数量。 选择出现多次查询词项地文档 胜者表：选取每个词项对应的权重最高的r篇文档作为胜者表，取每个词项的胜者表的并集中的TOP K作为文档集的TOP K结果。 …………………………………………… IR评价 IR 评价指标 效率（时间开销、空间开销、响应速度） 效果（准确率、召回率、F值） 覆盖率 访问量 更新速度 …… 省略关于召回率、准确率的说明。 基于调和平均计算出来的F值，可以看成是平滑的最小值函数，无论是P还是R，如果非常低，结果上都应该表现出相应的惩罚，而使用平均值或者精确率都不能达到这样的效果。 由于和查询相关的文档在文档集中占少数，即使对查询不做任何结果返回，精确率也能很高。 聚类 分类 分割式（Partitional） 分层式(Hierarchical) Definition: The process of grouping a set of objects into classes of similar objects. Success Criteria: Documents within a cluster should be similar Documents from different clusters should be dissimilar 以上两者的评判都取决与presentation document的选择和相似度的计算方法。 以上是比较通用的判断聚类是否成功的方法，还有通过一个聚类内部纯度的方法来判断，这里就不赘述。 聚类假设：在同一个聚类中的文档，就需求信息的相关性而言，它们的行为类似。$$\\longrightarrow$$ 所以，若要提高搜索返回结果的相关性，我们可以从以下两各方面入手： 我们需要在语料库中提前将文本聚类好 返回查询匹配文档所在聚类的其它所有文档 聚类在信息检索里的应用： 分析和导航整个语料库；提供了一个更好的用户接口，可以实现不打字也能搜索。 提高搜索结果的召回率。 对搜索结果有更好的导航 基于聚类的检索速度更快。 以下是聚类检索的例子，通过下图能对以上所说的应用有更直观的理解。 ISSUES FOR CLUSTERING 选出聚类的代表文档 评判文档之间相似度的方式 选择聚类的个数 固定值 由具体数据决定 文档相似度计量 文档相似度更多体现在文档的语义、内容上的相似性，计量与一相似性的一种可行方法是词量统计相似性。将每个文档视作一个向量，我们计量文档间的相似度即是计算向量间的距离，我们在表达的时候虽然常说的是“欧氏距离”，但在实际应用中，我们将会用到的是“余弦距离”。 分割式聚类我们将文档只做一次聚类，将文档一次分割为多个聚类，但是这会带来聚类个数的选定问题。分层聚类有两种，一种自上而下，一种至下而上，前者会迭代地聚类，将聚类结果再次聚类得到更精确的类，后者反之。 几种判断聚类间距离的策略： 本博客不对聚类做更多说明，在应用中可使用Hard-Cluster或者Soft-Cluster,前者每个文档只属于一个聚类，后者每个文档可以属于多个聚类。在课程设计中我们使用的是K-MEANS算法，在实现中需要注意的是K-MEANS算法中不同的聚类起始点的选择对结果会造成很大的影响。在此也不对K-MEANS算法做具体介绍。 文本分类文本分类的任务描述： 已知文本T的描述D和已知分类集合C，试图通过函数$$\\gamma(D)$$将文本T分类到某个C的某个类中。 监督分类的任务描述： 已知文本T的描述D、已知分类集合C、训练集&lt;d,c&gt;$$\\in X\\times C$$，试图训练出分类器$$\\gamma :X\\rightarrow C$$和通过函数$$\\gamma(D)$$将文本T分类到某个C的某个类中。 分类方法 手工分类：直接；精度高；不易于处理大文件；成本高。 自动分类 统计/概率方法 本次总结中还有一些方面并未提及，包括摘要的选择，相关反馈还有分类方法中的朴素贝叶斯和文本分类评价。","link":"/%E4%BA%92%E8%81%94%E7%BD%91%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}],"tags":[{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"Image","slug":"Image","link":"/tags/Image/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"学习","slug":"学习","link":"/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"图像分割","slug":"图像分割","link":"/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"},{"name":"计算机组成原理","slug":"计算机组成原理","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"},{"name":"搜索引擎","slug":"搜索引擎","link":"/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"}],"categories":[{"name":"Vision","slug":"Vision","link":"/categories/Vision/"},{"name":"Study","slug":"Study","link":"/categories/Study/"},{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"CPlus","slug":"CPlus","link":"/categories/CPlus/"},{"name":"计算机组成原理","slug":"Study/计算机组成原理","link":"/categories/Study/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"},{"name":"数据库","slug":"Study/数据库","link":"/categories/Study/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]}